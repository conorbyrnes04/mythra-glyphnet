{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MYTHRA GLYPHNET - MODEL TRAINING SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "import replicate\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API token from environment\n",
    "api_token = os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "if api_token:\n",
    "    os.environ[\"REPLICATE_API_TOKEN\"] = api_token\n",
    "    print(f\"‚úÖ API token loaded from .env (ends with: ...{api_token[-8:]})\")\n",
    "else:\n",
    "    print(\"‚ùå REPLICATE_API_TOKEN not found in .env file\")\n",
    "    print(\"üí° Please add your token to .env file: REPLICATE_API_TOKEN=your_token_here\")\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'base_dir': 'results/training_data',\n",
    "    'configs_dir': '../replicate/training',\n",
    "    'models_dir': 'results/trained_models',\n",
    "    'logs_dir': 'results/training_logs'\n",
    "}\n",
    "\n",
    "# Create training directories\n",
    "for dir_path in TRAINING_CONFIG.values():\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üöÄ MYTHRA GLYPHNET TRAINING SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ Purpose: Train specialized glyph generation models\")\n",
    "print(\"üìÅ Training data:\", TRAINING_CONFIG['base_dir'])\n",
    "print(\"‚öôÔ∏è Configs:\", TRAINING_CONFIG['configs_dir'])\n",
    "print(\"üè∑Ô∏è Models:\", TRAINING_CONFIG['models_dir'])\n",
    "print(\"üìä Logs:\", TRAINING_CONFIG['logs_dir'])\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING DATA MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_training_data():\n",
    "    \"\"\"Analyze available training data for model preparation\"\"\"\n",
    "    base_dir = Path(TRAINING_CONFIG['base_dir'])\n",
    "    glyph_sources = Path('results/glyphs')\n",
    "    \n",
    "    print(\"üìä TRAINING DATA ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check existing training data\n",
    "    if base_dir.exists():\n",
    "        for subdir in base_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                files = list(subdir.glob('*'))\n",
    "                image_files = [f for f in files if f.suffix in ['.png', '.webp', '.svg']]\n",
    "                print(f\"üìÅ {subdir.name}: {len(image_files)} images\")\n",
    "    \n",
    "    print(\"\\nüìà SOURCE GLYPH DATA:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check source glyphs\n",
    "    total_glyphs = 0\n",
    "    glyph_stats = {}\n",
    "    \n",
    "    if glyph_sources.exists():\n",
    "        for subdir in ['generic', 'dream', 'sumi', 'mythra']:\n",
    "            subdir_path = glyph_sources / subdir\n",
    "            if subdir_path.exists():\n",
    "                files = list(subdir_path.glob('*'))\n",
    "                image_files = [f for f in files if f.suffix in ['.png', '.webp', '.svg']]\n",
    "                glyph_stats[subdir] = len(image_files)\n",
    "                total_glyphs += len(image_files)\n",
    "                print(f\"üåÄ {subdir}: {len(image_files)} glyphs\")\n",
    "    \n",
    "    print(f\"\\nüìä TOTAL AVAILABLE: {total_glyphs} glyphs\")\n",
    "    \n",
    "    # Training recommendations\n",
    "    print(\"\\nüí° TRAINING RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_type, count in glyph_stats.items():\n",
    "        if count >= 50:\n",
    "            status = \"‚úÖ Ready for training\"\n",
    "        elif count >= 20:\n",
    "            status = \"‚ö†Ô∏è Minimal - consider generating more\"\n",
    "        else:\n",
    "            status = \"‚ùå Insufficient - need more data\"\n",
    "        print(f\"{model_type.upper()}: {count} images - {status}\")\n",
    "    \n",
    "    return glyph_stats\n",
    "\n",
    "def prepare_training_dataset(model_type, source_dir=None, target_name=None):\n",
    "    \"\"\"\n",
    "    Prepare training dataset for a specific model type\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'gglyphs', 'dglyphs', 'sumi', 'mythra'\n",
    "        source_dir: Override source directory\n",
    "        target_name: Override target dataset name\n",
    "    \"\"\"\n",
    "    if not source_dir:\n",
    "        source_mapping = {\n",
    "            'gglyphs': 'results/glyphs/generic',\n",
    "            'dglyphs': 'results/glyphs/dream',\n",
    "            'sumi': 'results/glyphs/sumi',\n",
    "            'mythra': 'results/glyphs/mythra'\n",
    "        }\n",
    "        source_dir = source_mapping.get(model_type)\n",
    "    \n",
    "    if not source_dir or not Path(source_dir).exists():\n",
    "        print(f\"‚ùå Source directory not found: {source_dir}\")\n",
    "        return None\n",
    "    \n",
    "    target_name = target_name or f\"{model_type}_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    target_dir = Path(TRAINING_CONFIG['base_dir']) / target_name\n",
    "    target_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy files\n",
    "    source_path = Path(source_dir)\n",
    "    files_copied = 0\n",
    "    \n",
    "    print(f\"üì¶ Preparing {model_type.upper()} training dataset...\")\n",
    "    print(f\"üìÇ Source: {source_dir}\")\n",
    "    print(f\"üéØ Target: {target_dir}\")\n",
    "    \n",
    "    for file_path in source_path.glob('*'):\n",
    "        if file_path.suffix in ['.png', '.webp', '.svg']:\n",
    "            target_file = target_dir / file_path.name\n",
    "            shutil.copy2(file_path, target_file)\n",
    "            files_copied += 1\n",
    "    \n",
    "    # Create dataset metadata\n",
    "    metadata = {\n",
    "        'model_type': model_type,\n",
    "        'source_directory': str(source_dir),\n",
    "        'target_directory': str(target_dir),\n",
    "        'files_count': files_copied,\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'file_types': ['.png', '.webp', '.svg'],\n",
    "        'purpose': f'Training data for {model_type} model'\n",
    "    }\n",
    "    \n",
    "    metadata_file = target_dir / 'dataset_metadata.yaml'\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        yaml.dump(metadata, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset prepared: {files_copied} files\")\n",
    "    print(f\"üìÑ Metadata saved: {metadata_file}\")\n",
    "    \n",
    "    return str(target_dir)\n",
    "\n",
    "def create_training_zip(dataset_dir, zip_name=None):\n",
    "    \"\"\"Create a ZIP file of training data for upload\"\"\"\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        print(f\"‚ùå Dataset directory not found: {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    zip_name = zip_name or f\"{dataset_path.name}.zip\"\n",
    "    zip_path = Path(TRAINING_CONFIG['base_dir']) / zip_name\n",
    "    \n",
    "    print(f\"üì¶ Creating training ZIP: {zip_name}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file_path in dataset_path.glob('*'):\n",
    "            if file_path.is_file() and file_path.suffix in ['.png', '.webp', '.svg']:\n",
    "                zipf.write(file_path, file_path.name)\n",
    "    \n",
    "    zip_size = zip_path.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"‚úÖ ZIP created: {zip_path}\")\n",
    "    print(f\"üìè Size: {zip_size:.2f} MB\")\n",
    "    \n",
    "    return str(zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error generating glyph: 'subject'\n"
     ]
    }
   ],
   "source": [
    "# Example generations - uncomment and modify as needed\n",
    "\n",
    "# SUMI model with sumi prompt template (default)\n",
    "# generate_glyph(\"dragon\", style=\"sumi\", model=\"sumi\")\n",
    "\n",
    "# MYTHRA model with mythra prompt template\n",
    "# generate_glyph(\"phoenix\", \"fire\", \"rebirth\", style=\"mythra\", model=\"mythra\")\n",
    "\n",
    "# Mix and match: SUMI model with generic template\n",
    "# generate_glyph(\"horse\", style=\"generic\", model=\"sumi\")\n",
    "\n",
    "# MYTHRA model with sumi-style template \n",
    "# generate_glyph(\"wolf\", \"moon\", \"wisdom\", style=\"sumi\", model=\"mythra\")\n",
    "\n",
    "# Quick test - generate a snake glyph using SUMI model\n",
    "generate_glyph(\"snake\", model=\"sumi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error generating glyph: 'subject'\n"
     ]
    }
   ],
   "source": [
    "# Example generations - uncomment and modify as needed\n",
    "\n",
    "# SUMI model with sumi prompt template (default)\n",
    "# generate_glyph(\"dragon\", style=\"sumi\", model=\"sumi\")\n",
    "\n",
    "# MYTHRA model with mythra prompt template\n",
    "# generate_glyph(\"phoenix\", \"fire\", \"rebirth\", style=\"mythra\", model=\"mythra\")\n",
    "\n",
    "# Mix and match: SUMI model with generic template\n",
    "# generate_glyph(\"horse\", style=\"generic\", model=\"sumi\")\n",
    "\n",
    "# MYTHRA model with sumi-style template \n",
    "# generate_glyph(\"wolf\", \"moon\", \"wisdom\", style=\"sumi\", model=\"mythra\")\n",
    "\n",
    "# Quick test - generate a snake glyph using SUMI model\n",
    "generate_glyph(\"snake\", model=\"sumi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error generating glyph: 'subject'\n"
     ]
    }
   ],
   "source": [
    "# Example generations - uncomment and modify as needed\n",
    "\n",
    "# SUMI model with sumi prompt template (default)\n",
    "# generate_glyph(\"dragon\", style=\"sumi\", model=\"sumi\")\n",
    "\n",
    "# MYTHRA model with mythra prompt template\n",
    "# generate_glyph(\"phoenix\", \"fire\", \"rebirth\", style=\"mythra\", model=\"mythra\")\n",
    "\n",
    "# Mix and match: SUMI model with generic template\n",
    "# generate_glyph(\"horse\", style=\"generic\", model=\"sumi\")\n",
    "\n",
    "# MYTHRA model with sumi-style template \n",
    "# generate_glyph(\"wolf\", \"moon\", \"wisdom\", style=\"sumi\", model=\"mythra\")\n",
    "\n",
    "# Quick test - generate a snake glyph using SUMI model\n",
    "generate_glyph(\"snake\", model=\"sumi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def load_training_config(config_name):\n",
    "    \"\"\"Load training configuration from YAML file\"\"\"\n",
    "    config_path = Path(TRAINING_CONFIG['configs_dir']) / f\"{config_name}.yaml\"\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        print(f\"‚ùå Config file not found: {config_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded config: {config_name}\")\n",
    "    return config\n",
    "\n",
    "def list_training_configs():\n",
    "    \"\"\"List all available training configurations\"\"\"\n",
    "    configs_dir = Path(TRAINING_CONFIG['configs_dir'])\n",
    "    \n",
    "    if not configs_dir.exists():\n",
    "        print(f\"‚ùå Configs directory not found: {configs_dir}\")\n",
    "        return []\n",
    "    \n",
    "    config_files = list(configs_dir.glob('*.yaml'))\n",
    "    \n",
    "    print(\"‚öôÔ∏è AVAILABLE TRAINING CONFIGURATIONS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    configs = []\n",
    "    for config_file in config_files:\n",
    "        config_name = config_file.stem\n",
    "        try:\n",
    "            with open(config_file, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            model_name = config.get('model_name', 'Unknown')\n",
    "            description = config.get('description', 'No description')\n",
    "            \n",
    "            print(f\"üìã {config_name}\")\n",
    "            print(f\"   Model: {model_name}\")\n",
    "            print(f\"   Description: {description}\")\n",
    "            print()\n",
    "            \n",
    "            configs.append(config_name)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {config_name}: {e}\")\n",
    "    \n",
    "    return configs\n",
    "\n",
    "def create_custom_config(model_name, base_config='gglyphs-config', custom_params=None):\n",
    "    \"\"\"Create a custom training configuration\"\"\"\n",
    "    \n",
    "    # Load base configuration\n",
    "    base_config_path = Path(TRAINING_CONFIG['configs_dir']) / f\"{base_config}.yaml\"\n",
    "    \n",
    "    if not base_config_path.exists():\n",
    "        print(f\"‚ùå Base config not found: {base_config}\")\n",
    "        return None\n",
    "    \n",
    "    with open(base_config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Update with custom parameters\n",
    "    config['model_name'] = model_name\n",
    "    config['created_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    if custom_params:\n",
    "        for key, value in custom_params.items():\n",
    "            if key in config:\n",
    "                config[key] = value\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Unknown parameter: {key}\")\n",
    "    \n",
    "    # Save custom config\n",
    "    custom_config_name = f\"{model_name.lower().replace(' ', '_')}_config\"\n",
    "    custom_config_path = Path(TRAINING_CONFIG['configs_dir']) / f\"{custom_config_name}.yaml\"\n",
    "    \n",
    "    with open(custom_config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"‚úÖ Created custom config: {custom_config_name}\")\n",
    "    print(f\"üìÅ Saved to: {custom_config_path}\")\n",
    "    \n",
    "    return custom_config_name\n",
    "\n",
    "def preview_training_config(config_name):\n",
    "    \"\"\"Preview a training configuration with key parameters highlighted\"\"\"\n",
    "    config = load_training_config(config_name)\n",
    "    \n",
    "    if not config:\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç TRAINING CONFIG PREVIEW: {config_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key parameters to highlight\n",
    "    key_params = [\n",
    "        'model_name', 'description', 'base_model', 'trigger_word',\n",
    "        'learning_rate', 'num_train_epochs', 'lora_rank', 'resolution',\n",
    "        'train_batch_size', 'lora_type'\n",
    "    ]\n",
    "    \n",
    "    for param in key_params:\n",
    "        if param in config:\n",
    "            print(f\"üìå {param}: {config[param]}\")\n",
    "    \n",
    "    print(\"\\nüéØ TRAINING PARAMETERS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    training_params = config.get('training_parameters', {})\n",
    "    for key, value in training_params.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    if 'validation_prompts' in config:\n",
    "        print(f\"\\n‚úÖ Validation prompts: {len(config['validation_prompts'])} prompts\")\n",
    "    \n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REPLICATE TRAINING INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "def start_training(config_name, dataset_path=None, destination=None):\n",
    "    \"\"\"\n",
    "    Start a training job on Replicate\n",
    "    \n",
    "    Args:\n",
    "        config_name: Name of training configuration\n",
    "        dataset_path: Path to training data (local or URL)\n",
    "        destination: Replicate model destination (username/model-name)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load configuration\n",
    "    config = load_training_config(config_name)\n",
    "    if not config:\n",
    "        return None\n",
    "    \n",
    "    print(f\"üöÄ STARTING TRAINING JOB\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"üìã Config: {config_name}\")\n",
    "    print(f\"üéØ Model: {config.get('model_name', 'Unknown')}\")\n",
    "    \n",
    "    if not destination:\n",
    "        print(\"‚ùå Error: destination required (e.g., 'username/model-name')\")\n",
    "        return None\n",
    "    \n",
    "    if not dataset_path:\n",
    "        print(\"‚ùå Error: dataset_path required\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Prepare training inputs from config\n",
    "        training_inputs = {\n",
    "            'input_images': dataset_path,\n",
    "            'trigger_word': config.get('trigger_word', 'GLYPH'),\n",
    "            'learning_rate': config.get('learning_rate', 1e-4),\n",
    "            'num_train_epochs': config.get('num_train_epochs', 1000),\n",
    "            'lora_rank': config.get('lora_rank', 16),\n",
    "            'resolution': config.get('resolution', 1024),\n",
    "            'train_batch_size': config.get('train_batch_size', 1),\n",
    "            'lora_type': config.get('lora_type', 'standard')\n",
    "        }\n",
    "        \n",
    "        # Add any additional parameters from config\n",
    "        if 'training_parameters' in config:\n",
    "            training_inputs.update(config['training_parameters'])\n",
    "        \n",
    "        print(f\"üìä Training inputs prepared:\")\n",
    "        for key, value in training_inputs.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Destination: {destination}\")\n",
    "        print(f\"üîÑ Starting training job...\")\n",
    "        \n",
    "        # Start training (commented out for safety - uncomment when ready)\n",
    "        # training = replicate.trainings.create(\n",
    "        #     version=\"recraft-ai/recraft-v3-svg:latest\",\n",
    "        #     input=training_inputs,\n",
    "        #     destination=destination\n",
    "        # )\n",
    "        \n",
    "        # For now, just return the prepared inputs\n",
    "        print(\"‚ö†Ô∏è Training job preparation complete!\")\n",
    "        print(\"üí° Uncomment the replicate.trainings.create() call to start actual training\")\n",
    "        \n",
    "        # Log training attempt\n",
    "        log_training_attempt(config_name, training_inputs, destination)\n",
    "        \n",
    "        return {\n",
    "            'config_name': config_name,\n",
    "            'inputs': training_inputs,\n",
    "            'destination': destination,\n",
    "            'status': 'prepared'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training error: {e}\")\n",
    "        return None\n",
    "\n",
    "def log_training_attempt(config_name, inputs, destination):\n",
    "    \"\"\"Log training attempt for tracking\"\"\"\n",
    "    log_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config_name': config_name,\n",
    "        'destination': destination,\n",
    "        'training_inputs': inputs,\n",
    "        'status': 'attempted'\n",
    "    }\n",
    "    \n",
    "    log_file = Path(TRAINING_CONFIG['logs_dir']) / f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml\"\n",
    "    \n",
    "    with open(log_file, 'w') as f:\n",
    "        yaml.dump(log_data, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"üìù Training attempt logged: {log_file}\")\n",
    "\n",
    "def monitor_training(training_id=None):\n",
    "    \"\"\"Monitor training progress (placeholder for future implementation)\"\"\"\n",
    "    print(\"üîç TRAINING MONITORING\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if not training_id:\n",
    "        print(\"üí° This function will monitor training progress\")\n",
    "        print(\"üí° Provide training_id when available from Replicate\")\n",
    "        return\n",
    "    \n",
    "    # Future implementation:\n",
    "    # training = replicate.trainings.get(training_id)\n",
    "    # print(f\"Status: {training.status}\")\n",
    "    # print(f\"Progress: {training.logs}\")\n",
    "    \n",
    "    print(f\"üîÑ Monitoring training: {training_id}\")\n",
    "    print(\"‚ö†Ô∏è Monitoring functionality to be implemented\")\n",
    "\n",
    "def list_training_logs():\n",
    "    \"\"\"List all training logs\"\"\"\n",
    "    logs_dir = Path(TRAINING_CONFIG['logs_dir'])\n",
    "    \n",
    "    if not logs_dir.exists():\n",
    "        print(\"üì≠ No training logs found\")\n",
    "        return []\n",
    "    \n",
    "    log_files = list(logs_dir.glob('training_log_*.yaml'))\n",
    "    \n",
    "    print(\"üìä TRAINING HISTORY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for log_file in sorted(log_files, reverse=True):\n",
    "        try:\n",
    "            with open(log_file, 'r') as f:\n",
    "                log_data = yaml.safe_load(f)\n",
    "            \n",
    "            timestamp = log_data.get('timestamp', 'Unknown')\n",
    "            config_name = log_data.get('config_name', 'Unknown')\n",
    "            destination = log_data.get('destination', 'Unknown')\n",
    "            status = log_data.get('status', 'Unknown')\n",
    "            \n",
    "            print(f\"üïê {timestamp}\")\n",
    "            print(f\"   Config: {config_name}\")\n",
    "            print(f\"   Destination: {destination}\")\n",
    "            print(f\"   Status: {status}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {log_file.name}: {e}\")\n",
    "    \n",
    "    return log_files\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üéØ TRAINING WORKFLOW EXAMPLES\n",
    "\n",
    "## Step 1: Analyze Available Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what training data is available\n",
    "stats = analyze_training_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Next steps based on analysis:\")\n",
    "print(\"1. If insufficient data, generate more glyphs using glyph_codex.ipynb\")\n",
    "print(\"2. If sufficient data, proceed to prepare training datasets\")\n",
    "print(\"3. Review available training configurations\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Review Training Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available training configurations\n",
    "available_configs = list_training_configs()\n",
    "\n",
    "print(\"\\nüîç DETAILED CONFIG PREVIEW:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Preview key configurations for gGlyphs and dGlyphs\n",
    "for config in ['gglyphs-config', 'dglyphs-config']:\n",
    "    if config in [c.replace('.yaml', '') for c in available_configs]:\n",
    "        print(f\"\\nüìã {config.upper()}:\")\n",
    "        preview_config = load_training_config(config)\n",
    "        if preview_config:\n",
    "            print(f\"   Trigger: {preview_config.get('trigger_word', 'N/A')}\")\n",
    "            print(f\"   Learning Rate: {preview_config.get('learning_rate', 'N/A')}\")\n",
    "            print(f\"   Epochs: {preview_config.get('num_train_epochs', 'N/A')}\")\n",
    "            print(f\"   LoRA Rank: {preview_config.get('lora_rank', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {config} not found - may need to be created\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Prepare Training Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training datasets for different model types\n",
    "\n",
    "print(\"üì¶ PREPARING TRAINING DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uncomment to prepare datasets:\n",
    "\n",
    "# 1. Prepare gGlyphs (generic) dataset\n",
    "# gglyphs_dataset = prepare_training_dataset('gglyphs')\n",
    "# if gglyphs_dataset:\n",
    "#     gglyphs_zip = create_training_zip(gglyphs_dataset, 'gglyphs_training.zip')\n",
    "\n",
    "# 2. Prepare dGlyphs (dream) dataset  \n",
    "# dglyphs_dataset = prepare_training_dataset('dglyphs')\n",
    "# if dglyphs_dataset:\n",
    "#     dglyphs_zip = create_training_zip(dglyphs_dataset, 'dglyphs_training.zip')\n",
    "\n",
    "# 3. Prepare SUMI style dataset\n",
    "# sumi_dataset = prepare_training_dataset('sumi')\n",
    "# if sumi_dataset:\n",
    "#     sumi_zip = create_training_zip(sumi_dataset, 'sumi_training.zip')\n",
    "\n",
    "# 4. Prepare MYTHRA style dataset\n",
    "# mythra_dataset = prepare_training_dataset('mythra')  \n",
    "# if mythra_dataset:\n",
    "#     mythra_zip = create_training_zip(mythra_dataset, 'mythra_training.zip')\n",
    "\n",
    "print(\"üí° Uncomment the lines above to prepare your training datasets!\")\n",
    "print(\"üìÅ Datasets will be created in:\", TRAINING_CONFIG['base_dir'])\n",
    "print(\"üì¶ ZIP files will be ready for upload to Replicate\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Start Training Jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training jobs for new models\n",
    "\n",
    "print(\"üöÄ TRAINING JOB PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example training job setups (uncomment when ready):\n",
    "\n",
    "# 1. Train gGlyphs model\n",
    "# gglyphs_training = start_training(\n",
    "#     config_name='gglyphs-config',\n",
    "#     dataset_path='path/to/gglyphs_training.zip',  # Upload ZIP to cloud storage first\n",
    "#     destination='yourusername/gglyphs-model'\n",
    "# )\n",
    "\n",
    "# 2. Train dGlyphs model\n",
    "# dglyphs_training = start_training(\n",
    "#     config_name='dglyphs-config', \n",
    "#     dataset_path='path/to/dglyphs_training.zip',  # Upload ZIP to cloud storage first\n",
    "#     destination='yourusername/dglyphs-model'\n",
    "# )\n",
    "\n",
    "print(\"üí° TRAINING WORKFLOW:\")\n",
    "print(\"1. ‚úÖ Prepare your datasets (above)\")\n",
    "print(\"2. üì§ Upload ZIP files to cloud storage (Google Drive, S3, etc.)\")\n",
    "print(\"3. üîó Get public URLs for the ZIP files\")\n",
    "print(\"4. üöÄ Use start_training() with the URLs\")\n",
    "print(\"5. üîç Monitor progress with monitor_training()\")\n",
    "\n",
    "print(\"\\nüìã REQUIRED PARAMETERS:\")\n",
    "print(\"- config_name: Training configuration to use\")\n",
    "print(\"- dataset_path: Public URL to training ZIP file\")  \n",
    "print(\"- destination: Your Replicate model destination (username/model-name)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT:\")\n",
    "print(\"- Training ZIP files must be publicly accessible URLs\")\n",
    "print(\"- Make sure you have sufficient Replicate credits\")\n",
    "print(\"- Training can take several hours depending on dataset size\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Monitor Training & Manage Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor training progress and manage trained models\n",
    "\n",
    "print(\"üîç TRAINING MONITORING & MANAGEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check training history\n",
    "training_logs = list_training_logs()\n",
    "\n",
    "print(f\"\\nüìä Found {len(training_logs)} training log(s)\")\n",
    "\n",
    "# Monitor active training (when you have training IDs)\n",
    "print(\"\\nüîÑ ACTIVE TRAINING MONITORING:\")\n",
    "print(\"Use monitor_training('training_id') when you have active jobs\")\n",
    "\n",
    "# Example monitoring call:\n",
    "# monitor_training('training_abc123')\n",
    "\n",
    "print(\"\\nüéØ POST-TRAINING STEPS:\")\n",
    "print(\"1. üè∑Ô∏è Update MODELS dict in glyph_codex.ipynb with new model IDs\")\n",
    "print(\"2. üß™ Test new models with validation prompts\")\n",
    "print(\"3. üìä Compare quality with existing models\")\n",
    "print(\"4. üîÑ Iterate on training parameters if needed\")\n",
    "\n",
    "print(\"\\nüí° TIPS:\")\n",
    "print(\"- Save model IDs immediately after training completes\")\n",
    "print(\"- Test with known prompts to evaluate quality\")\n",
    "print(\"- Consider fine-tuning parameters for better results\")\n",
    "print(\"- Document model capabilities and limitations\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üõ†Ô∏è ADVANCED TRAINING UTILITIES\n",
    "\n",
    "## Custom Configuration Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom training configurations for specialized models\n",
    "\n",
    "print(\"üõ†Ô∏è CUSTOM CONFIGURATION CREATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example: Create a custom configuration for a hybrid model\n",
    "# custom_params = {\n",
    "#     'trigger_word': 'HYBRIDGLYPH',\n",
    "#     'learning_rate': 2e-4,\n",
    "#     'num_train_epochs': 1500,\n",
    "#     'lora_rank': 32,\n",
    "#     'description': 'Hybrid model combining gGlyph and dGlyph characteristics'\n",
    "# }\n",
    "\n",
    "# hybrid_config = create_custom_config(\n",
    "#     model_name='Hybrid Glyphs',\n",
    "#     base_config='gglyphs-config',\n",
    "#     custom_params=custom_params\n",
    "# )\n",
    "\n",
    "print(\"üí° CREATE CUSTOM CONFIGS FOR:\")\n",
    "print(\"- Specialized glyph styles (e.g., Celtic, Egyptian, Modern)\")\n",
    "print(\"- Different art styles (sketch, photorealistic, abstract)\")\n",
    "print(\"- Specific use cases (logos, tattoos, icons)\")\n",
    "print(\"- Experimental parameter combinations\")\n",
    "\n",
    "print(\"\\nüìù CUSTOM PARAMETER OPTIONS:\")\n",
    "print(\"- trigger_word: Unique activation word\")\n",
    "print(\"- learning_rate: Training speed (1e-5 to 1e-3)\")\n",
    "print(\"- num_train_epochs: Training duration (500-3000)\")\n",
    "print(\"- lora_rank: Model capacity (8-64)\")\n",
    "print(\"- resolution: Image size (512, 1024)\")\n",
    "print(\"- train_batch_size: Memory usage (1-4)\")\n",
    "\n",
    "# Preview any existing custom configs\n",
    "print(\"\\nüîç PREVIEW CUSTOM CONFIG:\")\n",
    "# Uncomment to preview:\n",
    "# preview_training_config('your_custom_config_name')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üéì TRAINING BEST PRACTICES & TROUBLESHOOTING\n",
    "\n",
    "## Training Guidelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì TRAINING BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üìä DATA REQUIREMENTS:\")\n",
    "print(\"‚úÖ gGlyphs: 50+ high-quality generic symbols\")\n",
    "print(\"‚úÖ dGlyphs: 50+ diverse amalgamated symbols\")  \n",
    "print(\"‚úÖ Style models: 30+ consistent style examples\")\n",
    "print(\"‚úÖ Image quality: High resolution, clean backgrounds\")\n",
    "print(\"‚úÖ Consistency: Similar composition and framing\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è PARAMETER TUNING:\")\n",
    "print(\"üî• High learning rate (1e-3): Fast training, risk of instability\")\n",
    "print(\"üêå Low learning rate (1e-5): Slow but stable training\")\n",
    "print(\"üéØ Recommended start: 1e-4 for most cases\")\n",
    "print()\n",
    "print(\"üìà LoRA Rank:\")\n",
    "print(\"   8-16: Simple styles, fast training\")\n",
    "print(\"   32-64: Complex styles, detailed features\")\n",
    "print()\n",
    "print(\"üîÑ Epochs:\")\n",
    "print(\"   500-1000: Quick experiments\")\n",
    "print(\"   1000-2000: Production quality\")\n",
    "print(\"   2000+: High-detail specialized models\")\n",
    "\n",
    "print(\"\\nüö® COMMON ISSUES & SOLUTIONS:\")\n",
    "print(\"‚ùå Overfitting: Reduce epochs or learning rate\")\n",
    "print(\"‚ùå Underfitting: Increase epochs or learning rate\")  \n",
    "print(\"‚ùå Memory errors: Reduce batch size or resolution\")\n",
    "print(\"‚ùå Poor quality: Check data consistency\")\n",
    "print(\"‚ùå Slow convergence: Increase learning rate carefully\")\n",
    "\n",
    "print(\"\\nüí∞ COST OPTIMIZATION:\")\n",
    "print(\"üí° Start with smaller datasets for testing\")\n",
    "print(\"üí° Use lower resolutions for initial experiments\")\n",
    "print(\"üí° Monitor training - stop early if converged\")\n",
    "print(\"üí° Reuse successful parameter combinations\")\n",
    "\n",
    "print(\"\\nüéØ SUCCESS METRICS:\")\n",
    "print(\"üìà Training loss decreases steadily\")\n",
    "print(\"üé® Generated samples improve over time\")\n",
    "print(\"‚úÖ Validation prompts produce expected results\")\n",
    "print(\"üîÑ Model generalizes to new prompts\")\n",
    "print(\"‚ö° Inference speed is acceptable\")\n",
    "\n",
    "print(\"\\nüìû SUPPORT & RESOURCES:\")\n",
    "print(\"üìö Replicate documentation: https://replicate.com/docs\")\n",
    "print(\"üí¨ Community: Replicate Discord/forums\")\n",
    "print(\"üõ†Ô∏è Training configs: replicate/training/ directory\")\n",
    "print(\"üìä Logs: results/training_logs/ directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Google Drive Link Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting Google Drive share link...\n",
      "‚úÖ Google Drive link converted successfully!\n",
      "üì§ Share link: https://drive.google.com/file/d/1Zz-dvpw6-Wtc2deuw...\n",
      "üì• Direct URL: https://drive.google.com/uc?export=download&id=1Zz...\n",
      "üéØ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üì§ GOOGLE DRIVE CONVERTER & TRAINING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def convert_gdrive_link(share_url):\n",
    "    \"\"\"Convert Google Drive share link to direct download URL\"\"\"\n",
    "    if 'drive.google.com/file/d/' in share_url:\n",
    "        # Extract file ID from share URL\n",
    "        file_id = share_url.split('/file/d/')[1].split('/')[0]\n",
    "        direct_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "        return direct_url\n",
    "    elif 'drive.google.com/drive/folders/' in share_url:\n",
    "        print(\"‚ùå This is a FOLDER link, not a FILE link!\")\n",
    "        print(\"üí° For training, you need to:\")\n",
    "        print(\"   1. Create a ZIP file of your Midjourney images\")\n",
    "        print(\"   2. Upload the ZIP file to Google Drive\")\n",
    "        print(\"   3. Share the ZIP FILE (not the folder)\")\n",
    "        print(\"   4. Use the ZIP file's share link here\")\n",
    "        print(\"\\nüìã Example ZIP file link format:\")\n",
    "        print(\"   https://drive.google.com/file/d/1ABC123DEF456/view?usp=sharing\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"‚ùå Invalid Google Drive share URL format\")\n",
    "        print(\"üí° Expected format: https://drive.google.com/file/d/FILE_ID/view?usp=sharing\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# üì§ PASTE YOUR GOOGLE DRIVE SHARE LINK HERE\n",
    "# =============================================================================\n",
    "\n",
    "# PASTE YOUR GOOGLE DRIVE ZIP FILE SHARE LINK BELOW (between the quotes):\n",
    "MIDJOURNEY_ZIP_SHARE_LINK = \"https://drive.google.com/file/d/1Zz-dvpw6-Wtc2deuwAp5B6SIoRurGp26/view?usp=drive_link\"\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: This must be a ZIP FILE link, not a folder link!\n",
    "# \n",
    "# STEPS TO GET THE RIGHT LINK:\n",
    "# 1. Create a ZIP file containing your Midjourney images\n",
    "# 2. Upload the ZIP file to Google Drive  \n",
    "# 3. Right-click the ZIP file ‚Üí Share ‚Üí Copy link\n",
    "# 4. Paste that link above\n",
    "#\n",
    "# Example ZIP file format: \"https://drive.google.com/file/d/1ABC123DEF456GHI789/view?usp=sharing\"\n",
    "# NOT a folder format: \"https://drive.google.com/drive/folders/...\"\n",
    "\n",
    "# Convert to direct download URL\n",
    "print(\"üîÑ Converting Google Drive share link...\")\n",
    "if MIDJOURNEY_ZIP_SHARE_LINK:\n",
    "    MIDJOURNEY_ZIP_DIRECT_URL = convert_gdrive_link(MIDJOURNEY_ZIP_SHARE_LINK)\n",
    "    \n",
    "    if MIDJOURNEY_ZIP_DIRECT_URL:\n",
    "        print(\"‚úÖ Google Drive link converted successfully!\")\n",
    "        print(f\"üì§ Share link: {MIDJOURNEY_ZIP_SHARE_LINK[:50]}...\")\n",
    "        print(f\"üì• Direct URL: {MIDJOURNEY_ZIP_DIRECT_URL[:50]}...\")\n",
    "        print(\"üéØ Ready for training!\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to convert Google Drive link\")\n",
    "        print(\"üí° Please check your share link format\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please paste your Google Drive share link in the MIDJOURNEY_ZIP_SHARE_LINK variable above\")\n",
    "    MIDJOURNEY_ZIP_DIRECT_URL = None\n",
    "\n",
    "# Test the URL (optional)\n",
    "def test_training_url(url):\n",
    "    \"\"\"Quick test if the URL is accessible\"\"\"\n",
    "    if not url:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        response = requests.head(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ URL is accessible!\")\n",
    "            content_length = response.headers.get('content-length')\n",
    "            if content_length:\n",
    "                size_mb = int(content_length) / (1024 * 1024)\n",
    "                print(f\"üì¶ File size: {size_mb:.1f} MB\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå URL returned status code: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing URL: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to test your URL:\n",
    "# if MIDJOURNEY_ZIP_DIRECT_URL:\n",
    "#     test_training_url(MIDJOURNEY_ZIP_DIRECT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ READY TO START MIDJOURNEY CODEX TRAINING\n",
      "============================================================\n",
      "üé® Training Universal gGlyph Codex...\n",
      "üìä Dataset URL: https://drive.google.com/uc?export=download&id=1Zz...\n",
      "üéØ Will create model: conorbyrnes04/meru\n",
      "\n",
      "üí° TO START TRAINING:\n",
      "1. ‚úÖ Make sure your Google Drive link is set above\n",
      "2. ‚úÖ Verify the URL is accessible (uncomment test)\n",
      "3. ‚úÖ Update the destination username above\n",
      "4. üöÄ Remove the triple quotes around start_training() call\n",
      "5. ‚ñ∂Ô∏è Run this cell!\n",
      "\n",
      "‚è±Ô∏è TRAINING DETAILS:\n",
      "- Duration: ~30-90 minutes\n",
      "- Cost: ~$0.50-$3.00 in Replicate credits\n",
      "- Output: Custom SVG glyph model trained on your Midjourney style\n",
      "- Base Model: recraft-ai/recraft-v3-svg (SVG optimized)\n",
      "- Trigger Word: meru\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# üé® MIDJOURNEY CODEX TRAINING - READY TO USE\n",
    "# =============================================================================\n",
    "\n",
    "if MIDJOURNEY_ZIP_DIRECT_URL:\n",
    "    print(\"üöÄ READY TO START MIDJOURNEY CODEX TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Train gGlyph codex model using your Midjourney images\n",
    "    print(\"üé® Training Universal gGlyph Codex...\")\n",
    "    print(f\"üìä Dataset URL: {MIDJOURNEY_ZIP_DIRECT_URL[:50]}...\")\n",
    "    print(f\"üéØ Will create model: conorbyrnes04/meru\")\n",
    "    \n",
    "    # START TRAINING - Uncomment the lines below when ready:\n",
    "    \"\"\"\n",
    "    midjourney_codex_training = start_training(\n",
    "        config_name='midjourney-svg-config',\n",
    "        dataset_path=MIDJOURNEY_ZIP_DIRECT_URL,\n",
    "        destination='conorbyrnes04/meru'  # MERU model\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüí° TO START TRAINING:\")\n",
    "    print(\"1. ‚úÖ Make sure your Google Drive link is set above\")\n",
    "    print(\"2. ‚úÖ Verify the URL is accessible (uncomment test)\")\n",
    "    print(\"3. ‚úÖ Update the destination username above\")\n",
    "    print(\"4. üöÄ Remove the triple quotes around start_training() call\")\n",
    "    print(\"5. ‚ñ∂Ô∏è Run this cell!\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è TRAINING DETAILS:\")\n",
    "    print(\"- Duration: ~30-90 minutes\")\n",
    "    print(\"- Cost: ~$0.50-$3.00 in Replicate credits\")\n",
    "    print(\"- Output: Custom SVG glyph model trained on your Midjourney style\")\n",
    "    print(\"- Base Model: recraft-ai/recraft-v3-svg (SVG optimized)\")\n",
    "    print(\"- Trigger Word: meru\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Please set your Google Drive link first in the cell above\")\n",
    "    print(\"üí° Paste your share link in MIDJOURNEY_ZIP_SHARE_LINK variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
